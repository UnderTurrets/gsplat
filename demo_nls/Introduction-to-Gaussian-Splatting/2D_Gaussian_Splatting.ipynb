{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "6czy2W0_0owD",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:48.046689Z",
     "start_time": "2024-07-11T09:09:47.004307Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import imageio\n",
    "import yaml\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import requests"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVURVWiLA_ze"
   },
   "source": [
    "## 2D Gaussian Splatting ##\n",
    "\n",
    "The core function for generating 2D Gaussian Splatting. The core mechanic of this function is as follow\n",
    "\n",
    "> Constructs a 2D Gaussian kernel using provided batch of sigma_x, sigma_y, and rho\n",
    "\\begin{equation} \\sigma_x, \\sigma_y, \\rho\\ \\end{equation}\n",
    "\n",
    "\n",
    "> Normalises and reshapes the kernel to RGB channels, pads to match the image size, and translates based on given coords. Basically poutting the relevant kernel in the relevant coordinate position.\n",
    "\n",
    "\n",
    "> Multiplies the RGB kernels with given colours, sums up the layers, and returns the final clamped and permuted image.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9BUCJRA--1zw",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:48.055584Z",
     "start_time": "2024-07-11T09:09:48.047912Z"
    }
   },
   "source": [
    "def generate_2D_gaussian_splatting(kernel_size, sigma_x, sigma_y, rho, coords, colours, image_size=(256, 256, 3), device=\"cpu\"):\n",
    "\n",
    "    batch_size = colours.shape[0]\n",
    "\n",
    "    sigma_x = sigma_x.view(batch_size, 1, 1)\n",
    "    sigma_y = sigma_y.view(batch_size, 1, 1)\n",
    "    rho = rho.view(batch_size, 1, 1)\n",
    "\n",
    "    covariance = torch.stack(\n",
    "        [torch.stack([sigma_x**2, rho*sigma_x*sigma_y], dim=-1),\n",
    "        torch.stack([rho*sigma_x*sigma_y, sigma_y**2], dim=-1)],\n",
    "        dim=-2\n",
    "    )\n",
    "\n",
    "    # Check for positive semi-definiteness\n",
    "    determinant = (sigma_x**2) * (sigma_y**2) - (rho * sigma_x * sigma_y)**2\n",
    "    if (determinant <= 0).any():\n",
    "        raise ValueError(\"Covariance matrix must be positive semi-definite\")\n",
    "\n",
    "    inv_covariance = torch.inverse(covariance)\n",
    "\n",
    "    # Choosing quite a broad range for the distribution [-5,5] to avoid any clipping\n",
    "    start = torch.tensor([-5.0], device=device).view(-1, 1)\n",
    "    end = torch.tensor([5.0], device=device).view(-1, 1)\n",
    "    base_linspace = torch.linspace(0, 1, steps=kernel_size, device=device)\n",
    "    ax_batch = start + (end - start) * base_linspace\n",
    "\n",
    "    # Expanding dims for broadcasting\n",
    "    ax_batch_expanded_x = ax_batch.unsqueeze(-1).expand(-1, -1, kernel_size)\n",
    "    ax_batch_expanded_y = ax_batch.unsqueeze(1).expand(-1, kernel_size, -1)\n",
    "\n",
    "    # Creating a batch-wise meshgrid using broadcasting\n",
    "    xx, yy = ax_batch_expanded_x, ax_batch_expanded_y\n",
    "\n",
    "    xy = torch.stack([xx, yy], dim=-1)\n",
    "    z = torch.einsum('b...i,b...ij,b...j->b...', xy, -0.5 * inv_covariance, xy)\n",
    "    kernel = torch.exp(z) / (2 * torch.tensor(np.pi, device=device) * torch.sqrt(torch.det(covariance)).view(batch_size, 1, 1))\n",
    "\n",
    "\n",
    "    kernel_max_1, _ = kernel.max(dim=-1, keepdim=True)  # Find max along the last dimension\n",
    "    kernel_max_2, _ = kernel_max_1.max(dim=-2, keepdim=True)  # Find max along the second-to-last dimension\n",
    "    kernel_normalized = kernel / kernel_max_2\n",
    "\n",
    "    kernel_reshaped = kernel_normalized.repeat(1, 3, 1).view(batch_size * 3, kernel_size, kernel_size)\n",
    "    kernel_rgb = kernel_reshaped.unsqueeze(0).reshape(batch_size, 3, kernel_size, kernel_size)\n",
    "\n",
    "    # Calculating the padding needed to match the image size\n",
    "    pad_h = image_size[0] - kernel_size\n",
    "    pad_w = image_size[1] - kernel_size\n",
    "\n",
    "    if pad_h < 0 or pad_w < 0:\n",
    "        raise ValueError(\"Kernel size should be smaller or equal to the image size.\")\n",
    "\n",
    "    # Adding padding to make kernel size equal to the image size\n",
    "    padding = (pad_w // 2, pad_w // 2 + pad_w % 2,  # padding left and right\n",
    "               pad_h // 2, pad_h // 2 + pad_h % 2)  # padding top and bottom\n",
    "\n",
    "    kernel_rgb_padded = torch.nn.functional.pad(kernel_rgb, padding, \"constant\", 0)\n",
    "\n",
    "    # Extracting shape information\n",
    "    b, c, h, w = kernel_rgb_padded.shape\n",
    "\n",
    "    # Create a batch of 2D affine matrices\n",
    "    theta = torch.zeros(b, 2, 3, dtype=torch.float32, device=device)\n",
    "    theta[:, 0, 0] = 1.0\n",
    "    theta[:, 1, 1] = 1.0\n",
    "    theta[:, :, 2] = coords\n",
    "\n",
    "    # Creating grid and performing grid sampling\n",
    "    grid = F.affine_grid(theta, size=(b, c, h, w), align_corners=True)\n",
    "    kernel_rgb_padded_translated = F.grid_sample(kernel_rgb_padded, grid, align_corners=True)\n",
    "\n",
    "    rgb_values_reshaped = colours.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    final_image_layers = rgb_values_reshaped * kernel_rgb_padded_translated\n",
    "    final_image = final_image_layers.sum(dim=0)\n",
    "    final_image = torch.clamp(final_image, 0, 1)\n",
    "    final_image = final_image.permute(1,2,0)\n",
    "\n",
    "    return final_image\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GoeRQmOh6ty"
   },
   "source": [
    "# Example usage\n",
    "\n",
    "colours:(R,G,B)\n",
    "\n",
    "coordinates:(x,y), x:<--  y:向上"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "tpq6PP1ce8Kb",
    "outputId": "c1932891-76e1-480e-d650-db3a78fd6a54",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:48.777214Z",
     "start_time": "2024-07-11T09:09:48.184606Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\")\n",
    "kernel_size = 200  # You can adjust the kernel size as needed\n",
    "rho = torch.tensor([0.0, 0.0, -0.5], device=device)\n",
    "sigma_x = torch.tensor([2.0, 0.5, 0.5], device=device)\n",
    "sigma_y = torch.tensor([2.0, 0.5, 1.5], device=device)\n",
    "vectors = torch.tensor([(-0.5, -0.5), (0.8, 0.8), (0.5, 0.5)], device=device)\n",
    "colours = torch.tensor([(1.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 0.0, 1.0)], device=device)\n",
    "img_size = (1080, 1920, 3)\n",
    "\n",
    "final_image = generate_2D_gaussian_splatting(kernel_size, sigma_x, sigma_y, rho, vectors, colours, img_size, device=device)\n",
    "\n",
    "plt.imshow(final_image.detach().cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnB8nYi7bliG"
   },
   "source": [
    "## Structural Similarity Index (SSIM) ##\n",
    "This function calculates the combined loss by taking the weighted sum of L1 (mean absolute error) loss and the SSIM-based loss, controlled by lambda_param. This approach might be used to balance the importance of pixel-level accuracy (L1) and perceptual quality (SSIM) in the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WoAiMgO52zIl",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:50.683903Z",
     "start_time": "2024-07-11T09:09:50.673203Z"
    }
   },
   "source": [
    "def create_window(window_size, channel):\n",
    "    def gaussian(window_size, sigma):\n",
    "        gauss = torch.exp(torch.tensor([-(x - window_size//2)**2/float(2*sigma**2) for x in range(window_size)]))\n",
    "        return gauss/gauss.sum()\n",
    "\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = torch.autograd.Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "\n",
    "    return window\n",
    "\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "\n",
    "\n",
    "    # Assuming the image is of shape [N, C, H, W]\n",
    "    (_, _, channel) = img1.size()\n",
    "\n",
    "    img1 = img1.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    img2 = img2.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "    # Parameters for SSIM\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
    "\n",
    "    SSIM_numerator = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))\n",
    "    SSIM_denominator = ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "    SSIM = SSIM_numerator / SSIM_denominator\n",
    "\n",
    "    return torch.clamp((1 - SSIM) / 2, 0, 1)\n",
    "\n",
    "def d_ssim_loss(img1, img2, window_size=11, size_average=True):\n",
    "    return ssim(img1, img2, window_size, size_average).mean()\n",
    "\n",
    "# Combined Loss\n",
    "def combined_loss(pred, target, lambda_param=0.5):\n",
    "    l1loss = nn.L1Loss()\n",
    "    return (1 - lambda_param) * l1loss(pred, target) + lambda_param * d_ssim_loss(pred, target)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQtXWcj_sgoh"
   },
   "source": [
    "## Download Data ##\n",
    "This is mainly to make sure the config and image file exist and downloaded from the repository especially if you running it on colab"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ranC7rJSsgoh",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:53.691031Z",
     "start_time": "2024-07-11T09:09:51.760882Z"
    }
   },
   "source": [
    "url1 = 'https://raw.githubusercontent.com/OutofAi/2D-Gaussian-Splatting/main/Image-01.png'\n",
    "filename1 = url1.split('/')[-1]\n",
    "response1 = requests.get(url1)\n",
    "with open(filename1, 'wb') as f:\n",
    "    f.write(response1.content)\n",
    "\n",
    "url2 = 'https://raw.githubusercontent.com/OutofAi/2D-Gaussian-Splatting/main/config.yml'\n",
    "filename2 = url2.split('/')[-1]\n",
    "response2 = requests.get(url2)\n",
    "with open(filename2, 'wb') as f:\n",
    "    f.write(response2.content)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i279V2QJVyN"
   },
   "source": [
    "## Load Config ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xkAtK0co4o_R",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:53.697142Z",
     "start_time": "2024-07-11T09:09:53.692305Z"
    }
   },
   "source": [
    "# Read the config.yml file\n",
    "with open('config.yml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# Extract values from the loaded config\n",
    "KERNEL_SIZE = config[\"KERNEL_SIZE\"]\n",
    "image_size = tuple(config[\"image_size\"])\n",
    "primary_samples = config[\"primary_samples\"]\n",
    "backup_samples = config[\"backup_samples\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "densification_interval = config[\"densification_interval\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "image_file_name = config[\"image_file_name\"]\n",
    "display_interval = config[\"display_interval\"]\n",
    "grad_threshold = config[\"gradient_threshold\"]\n",
    "gauss_threshold = config[\"gaussian_threshold\"]\n",
    "display_loss = config[\"display_loss\"]\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQsMHUffxVNE"
   },
   "source": [
    "## Prepate the points ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U_v9rBeY_j1C",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:54.273412Z",
     "start_time": "2024-07-11T09:09:54.269657Z"
    }
   },
   "source": [
    "def give_required_data(input_coords, image_size):\n",
    "\n",
    "  # normalising pixel coordinates [-1,1]\n",
    "  coords = torch.tensor(input_coords / [image_size[0],image_size[1]], device=device).float()\n",
    "  center_coords_normalized = torch.tensor([0.5, 0.5], device=device).float()\n",
    "  coords = (center_coords_normalized - coords) * 2.0\n",
    "\n",
    "  # Fetching the colour of the pixels in each coordinates\n",
    "  colour_values = [image_array[coord[1], coord[0]] for coord in input_coords]\n",
    "  colour_values_np = np.array(colour_values)\n",
    "  colour_values_tensor =  torch.tensor(colour_values_np, device=device).float()\n",
    "\n",
    "  return colour_values_tensor, coords"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DL4PDDwO_lGD",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:00.357459Z",
     "start_time": "2024-07-11T09:10:00.311667Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_samples = primary_samples + backup_samples\n",
    "\n",
    "PADDING = KERNEL_SIZE // 2\n",
    "image_path = image_file_name\n",
    "original_image = Image.open(image_path)\n",
    "original_image = original_image.resize((image_size[0],image_size[0]))\n",
    "original_image = original_image.convert('RGB')\n",
    "original_array = np.array(original_image)\n",
    "original_array = original_array / 255.0\n",
    "width, height, _ = original_array.shape\n",
    "\n",
    "image_array = original_array\n",
    "target_tensor = torch.tensor(image_array, dtype=torch.float32, device=device)\n",
    "coords = np.random.randint(0, [width, height], size=(num_samples, 2))\n",
    "random_pixel_means = torch.tensor(coords, device=device)\n",
    "pixels = [image_array[coord[0], coord[1]] for coord in coords]\n",
    "pixels_np = np.array(pixels)\n",
    "random_pixels =  torch.tensor(pixels_np, device=device)\n",
    "\n",
    "colour_values, pixel_coords = give_required_data(coords, image_size)\n",
    "\n",
    "pixel_coords = torch.atanh(pixel_coords)\n",
    "\n",
    "sigma_values = torch.rand(num_samples, 2, device=device)\n",
    "rho_values = 2 * torch.rand(num_samples, 1, device=device) - 1\n",
    "alpha_values = torch.ones(num_samples, 1, device=device)\n",
    "W_values = torch.cat([sigma_values, rho_values, alpha_values, colour_values, pixel_coords], dim=1)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qyY0kbHiAaHh",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:00.823435Z",
     "start_time": "2024-07-11T09:10:00.820628Z"
    }
   },
   "source": [
    "starting_size = primary_samples\n",
    "left_over_size = backup_samples\n",
    "persistent_mask = torch.cat([torch.ones(starting_size, dtype=bool),torch.zeros(left_over_size, dtype=bool)], dim=0)\n",
    "current_marker = starting_size"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iv1odiTTxrIw",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:04.529702Z",
     "start_time": "2024-07-11T09:10:04.527272Z"
    }
   },
   "source": [
    "# Get current date and time as string\n",
    "now = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "\n",
    "# Create a directory with the current date and time as its name\n",
    "directory = f\"{now}\"\n",
    "os.makedirs(directory, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NwsEfutByA7N",
    "ExecuteTime": {
     "end_time": "2024-07-11T10:55:01.156085Z",
     "start_time": "2024-07-11T10:55:01.153684Z"
    }
   },
   "source": [
    "W = nn.Parameter(W_values)\n",
    "optimizer = Adam([W], lr=learning_rate)\n",
    "loss_history = []"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgFNaM62Lb6g"
   },
   "source": [
    "## Training Loop ##"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aiadZrbwAdu6",
    "outputId": "59139834-fa57-49af-842b-c985232cc5a4",
    "ExecuteTime": {
     "end_time": "2024-07-09T08:35:57.387324Z",
     "start_time": "2024-07-09T08:35:56.820142Z"
    }
   },
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #find indices to remove and update the persistent mask\n",
    "    if epoch % (densification_interval + 1) == 0 and epoch > 0:\n",
    "        indices_to_remove = (torch.sigmoid(W[:, 3]) < 0.01).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(indices_to_remove) > 0:\n",
    "          print(f\"number of pruned points: {len(indices_to_remove)}\")\n",
    "\n",
    "        persistent_mask[indices_to_remove] = False\n",
    "\n",
    "        # Zero-out parameters and their gradients at every epoch using the persistent mask\n",
    "        W.data[~persistent_mask] = 0.0\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    output = W[persistent_mask]\n",
    "\n",
    "    batch_size = output.shape[0]\n",
    "\n",
    "    sigma_x = torch.sigmoid(output[:, 0])\n",
    "    sigma_y = torch.sigmoid(output[:, 1])\n",
    "    rho = torch.tanh(output[:, 2])\n",
    "    alpha = torch.sigmoid(output[:, 3])\n",
    "    colours = torch.sigmoid(output[:, 4:7])\n",
    "    pixel_coords = torch.tanh(output[:, 7:9])\n",
    "\n",
    "    colours_with_alpha  = colours * alpha.view(batch_size, 1)\n",
    "    g_tensor_batch = generate_2D_gaussian_splatting(KERNEL_SIZE, sigma_x, sigma_y, rho, pixel_coords, colours_with_alpha, image_size, device)\n",
    "    loss = combined_loss(g_tensor_batch, target_tensor, lambda_param=0.2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply zeroing out of gradients at every epoch\n",
    "    if persistent_mask is not None:\n",
    "        W.grad.data[~persistent_mask] = 0.0\n",
    "\n",
    "    if epoch % densification_interval == 0 and epoch > 0:\n",
    "\n",
    "      # Calculate the norm of gradients\n",
    "      gradient_norms = torch.norm(W.grad[persistent_mask][:, 7:9], dim=1, p=2)\n",
    "      gaussian_norms = torch.norm(torch.sigmoid(W.data[persistent_mask][:, 0:2]), dim=1, p=2)\n",
    "\n",
    "      sorted_grads, sorted_grads_indices = torch.sort(gradient_norms, descending=True)\n",
    "      sorted_gauss, sorted_gauss_indices = torch.sort(gaussian_norms, descending=True)\n",
    "\n",
    "      large_gradient_mask = (sorted_grads > grad_threshold)\n",
    "      large_gradient_indices = sorted_grads_indices[large_gradient_mask]\n",
    "\n",
    "      large_gauss_mask = (sorted_gauss > gauss_threshold)\n",
    "      large_gauss_indices = sorted_gauss_indices[large_gauss_mask]\n",
    "\n",
    "      common_indices_mask = torch.isin(large_gradient_indices, large_gauss_indices)\n",
    "      common_indices = large_gradient_indices[common_indices_mask]\n",
    "      distinct_indices = large_gradient_indices[~common_indices_mask]\n",
    "\n",
    "      # Split points with large coordinate gradient and large gaussian values and descale their gaussian\n",
    "      if len(common_indices) > 0:\n",
    "        print(f\"number of splitted points: {len(common_indices)}\")\n",
    "        start_index = current_marker + 1\n",
    "        end_index = current_marker + 1 + len(common_indices)\n",
    "        persistent_mask[start_index: end_index] = True\n",
    "        W.data[start_index:end_index, :] = W.data[common_indices, :]\n",
    "        scale_reduction_factor = 1.6\n",
    "        W.data[start_index:end_index, 0:2] /= scale_reduction_factor\n",
    "        W.data[common_indices, 0:2] /= scale_reduction_factor\n",
    "        current_marker = current_marker + len(common_indices)\n",
    "\n",
    "      # Clone it points with large coordinate gradient and small gaussian values\n",
    "      if len(distinct_indices) > 0:\n",
    "\n",
    "        print(f\"number of cloned points: {len(distinct_indices)}\")\n",
    "        start_index = current_marker + 1\n",
    "        end_index = current_marker + 1 + len(distinct_indices)\n",
    "        persistent_mask[start_index: end_index] = True\n",
    "        W.data[start_index:end_index, :] = W.data[distinct_indices, :]\n",
    "        current_marker = current_marker + len(distinct_indices)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if epoch % display_interval == 0:\n",
    "        num_subplots = 3 if display_loss else 2\n",
    "        fig_size_width = 18 if display_loss else 12\n",
    "\n",
    "        fig, ax = plt.subplots(1, num_subplots, figsize=(fig_size_width, 6))  # Adjust subplot to 1x3\n",
    "\n",
    "        generated_array = g_tensor_batch.cpu().detach().numpy()\n",
    "\n",
    "        ax[0].imshow(g_tensor_batch.cpu().detach().numpy())\n",
    "        ax[0].set_title('2D Gaussian Splatting')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(target_tensor.cpu().detach().numpy())\n",
    "        ax[1].set_title('Ground Truth')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        if display_loss:\n",
    "          ax[2].plot(range(epoch + 1), loss_history[:epoch + 1])\n",
    "          ax[2].set_title('Loss vs. Epochs')\n",
    "          ax[2].set_xlabel('Epoch')\n",
    "          ax[2].set_ylabel('Loss')\n",
    "          ax[2].set_xlim(0, num_epochs)  # Set x-axis limits\n",
    "\n",
    "        # Display the image\n",
    "        #plt.show(block=False)\n",
    "        plt.subplots_adjust(wspace=0.1)  # Adjust this value to your preference\n",
    "        plt.pause(0.1)  # Brief pause\n",
    "\n",
    "        img = Image.fromarray((generated_array * 255).astype(np.uint8))\n",
    "\n",
    "        # Create filename\n",
    "        filename = f\"{epoch}.jpg\"\n",
    "\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Save the image\n",
    "        img.save(file_path)\n",
    "\n",
    "        fig.savefig(file_path, bbox_inches='tight')\n",
    "\n",
    "        plt.clf()  # Clear the current figure\n",
    "        plt.close()  # Close the current figure\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, on {len(output)} points\")"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Gradient tensor 0 does not have grad_fn. When calling loss.backward(), make sure the option create_graph is set to True.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 83\u001B[0m\n\u001B[1;32m     80\u001B[0m     W\u001B[38;5;241m.\u001B[39mdata[start_index:end_index, :] \u001B[38;5;241m=\u001B[39m W\u001B[38;5;241m.\u001B[39mdata[distinct_indices, :]\n\u001B[1;32m     81\u001B[0m     current_marker \u001B[38;5;241m=\u001B[39m current_marker \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(distinct_indices)\n\u001B[0;32m---> 83\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m loss_history\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m display_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/pyEnv/3DGS/lib/python3.8/site-packages/torch/optim/optimizer.py:391\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    387\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m             )\n\u001B[0;32m--> 391\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    394\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/pyEnv/3DGS/lib/python3.8/site-packages/torch_optimizer/adahessian.py:158\u001B[0m, in \u001B[0;36mAdahessian.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    154\u001B[0m             grads\u001B[38;5;241m.\u001B[39mappend(p\u001B[38;5;241m.\u001B[39mgrad)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;66;03m# get the Hessian diagonal\u001B[39;00m\n\u001B[0;32m--> 158\u001B[0m hut_traces \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_trace\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (p, group, grad, hut_trace) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m    161\u001B[0m     params, groups, grads, hut_traces\n\u001B[1;32m    162\u001B[0m ):\n\u001B[1;32m    164\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate[p]\n",
      "File \u001B[0;32m~/Desktop/pyEnv/3DGS/lib/python3.8/site-packages/torch_optimizer/adahessian.py:98\u001B[0m, in \u001B[0;36mAdahessian.get_trace\u001B[0;34m(self, params, grads)\u001B[0m\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m grad\u001B[38;5;241m.\u001B[39mgrad_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     94\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGradient tensor \u001B[39m\u001B[38;5;132;01m{:}\u001B[39;00m\u001B[38;5;124m does not have grad_fn. When \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     95\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcalling loss.backward(), make sure the option \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     96\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcreate_graph is set to True.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     97\u001B[0m         )\n\u001B[0;32m---> 98\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(i))\n\u001B[1;32m    100\u001B[0m v \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint_like(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m params\n\u001B[1;32m    107\u001B[0m ]\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# this is for distributed setting with single node and multi-gpus,\u001B[39;00m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;66;03m# for multi nodes setting, we have not support it yet.\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Gradient tensor 0 does not have grad_fn. When calling loss.backward(), make sure the option create_graph is set to True."
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hwm6e3_Wsgoj"
   },
   "source": [
    "#Calculate PSNR of the reconstructed image\n",
    "def psnr(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "psnr_value = psnr(g_tensor_batch, target_tensor)\n",
    "print(f\"PSNR: {psnr_value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_compression_ratio(original_image_size, num_gaussians):\n",
    "    # Size of original image in bytes\n",
    "    original_size_bytes = original_image_size[0] * original_image_size[1] * 24 / 8\n",
    "\n",
    "    # Size of compressed image in bytes\n",
    "    compressed_size_bytes = num_gaussians * 9 * 32 / 8\n",
    "\n",
    "    # Compression ratio\n",
    "    compression_ratio = original_size_bytes / compressed_size_bytes\n",
    "\n",
    "    return compression_ratio"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "compression_ratio = calculate_compression_ratio(image_size, current_marker)\n",
    "compression_ratio"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
